"""
vLLM Module Analyzer í…ŒìŠ¤íŠ¸
"""

import asyncio
from pathlib import Path
from module_analyzer_vllm import ModuleAnalyzer, LLMClient, HAS_HTTPX


async def test_basic_functionality():
    """ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (AI ì—†ì´)"""

    print("=" * 60)
    print("Module Analyzer vLLM - ê¸°ë³¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    current_dir = Path(__file__).parent

    # AI ì—†ì´ ë¶„ì„ê¸° ìƒì„±
    analyzer = ModuleAnalyzer(
        project_root=current_dir,
        use_ai=False,
        use_cache=False
    )

    print("\n1. Python íŒŒì¼ ì°¾ê¸° í…ŒìŠ¤íŠ¸...")
    python_files = analyzer._find_python_files(
        current_dir,
        exclude_patterns=["__pycache__", ".venv", "test_"]
    )
    print(f"   âœ“ {len(python_files)}ê°œ íŒŒì¼ ë°œê²¬")
    for f in python_files[:3]:
        print(f"     - {f.name}")

    print("\n2. ë‹¨ì¼ íŒŒì¼ ë¶„ì„ í…ŒìŠ¤íŠ¸ (AI ì—†ì´)...")
    if python_files:
        test_file = [f for f in python_files if f.name == "module_analyzer.py"]
        if test_file:
            test_file = test_file[0]
            print(f"   ë¶„ì„ ëŒ€ìƒ: {test_file.name}")

            result = await analyzer.analyze_file(test_file)

            print(f"   âœ“ ëª¨ë“ˆ ID: {result['module_id']}")
            print(f"   âœ“ í´ë˜ìŠ¤: {result['metrics']['num_classes']}ê°œ")
            print(f"   âœ“ í•¨ìˆ˜: {result['metrics']['num_functions']}ê°œ")
            print(f"   âœ“ ë³µì¡ë„: {result['summary']['complexity_level']}")
            print(f"   âœ“ ì„¤ëª…: {result['summary']['one_liner']}")

    print("\n" + "=" * 60)
    print("âœ… ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)


async def test_llm_connection():
    """LLM ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""

    print("\n" + "=" * 60)
    print("LLM ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    if not HAS_HTTPX:
        print("\nâŒ httpxê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜: pip install httpx")
        return

    print("\nğŸ”Œ http://localhost:8000 ì—°ê²° ì‹œë„...")

    try:
        llm = LLMClient(
            base_url="http://localhost:8000",
            model="meta-llama/Llama-3.2-3B-Instruct"
        )

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        if await llm.test_connection():
            print("   âœ… ì—°ê²° ì„±ê³µ!")

            # ê°„ë‹¨í•œ ì™„ì„± í…ŒìŠ¤íŠ¸
            print("\nğŸ“ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸...")
            prompt = """<|begin_of_text|><|start_header_id|>user<|end_header_id|>

Say "Hello from vLLM!" in JSON format:
{"message": "..."}
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
            response = await llm.complete(
                prompt=prompt,
                max_tokens=50,
                temperature=0.1
            )

            print(f"   ì‘ë‹µ: {response[:100]}...")

            print("\nâœ… LLM ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!")
            print("   ì´ì œ AI ë¶„ì„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            print("   python module_analyzer_vllm.py .")

        else:
            print("   âŒ ì—°ê²° ì‹¤íŒ¨")
            print("\nğŸ’¡ vLLM ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•˜ì„¸ìš”:")
            print("   python -m vllm.entrypoints.openai.api_server \\")
            print("       --model meta-llama/Llama-3.2-3B-Instruct \\")
            print("       --port 8000")

    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {e}")
        print("\nğŸ’¡ vLLM ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•˜ì„¸ìš”:")
        print("   python -m vllm.entrypoints.openai.api_server \\")
        print("       --model meta-llama/Llama-3.2-3B-Instruct \\")
        print("       --port 8000")

    print("=" * 60)


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

    # 1. ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    await test_basic_functionality()

    # 2. LLM ì—°ê²° í…ŒìŠ¤íŠ¸
    await test_llm_connection()

    print("\n" + "=" * 60)
    print("ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. vLLM ì„œë²„ ì‹œì‘:")
    print("   python -m vllm.entrypoints.openai.api_server \\")
    print("       --model meta-llama/Llama-3.2-3B-Instruct \\")
    print("       --port 8000")
    print("\n2. AI ë¶„ì„ ì‹¤í–‰:")
    print("   python module_analyzer_vllm.py .")
    print("\n3. ê²°ê³¼ í™•ì¸:")
    print("   cat docs/architecture/index.json | jq .")


if __name__ == '__main__':
    asyncio.run(main())
