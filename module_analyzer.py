"""
Module Analyzer: ì½”ë“œë² ì´ìŠ¤ ìë™ ë¶„ì„ ë° ë¬¸ì„œí™” ë„êµ¬

ì‚¬ìš©ë²•:
    python module_analyzer.py /path/to/project
    python module_analyzer.py /path/to/project --target v1
    python module_analyzer.py /path/to/project --no-ai
    python module_analyzer.py /path/to/project --no-cache
"""

import ast
import json
import asyncio
import time
import hashlib
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
import logging

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


# =============================================================================
# ìƒìˆ˜ ì •ì˜
# =============================================================================

class Config:
    """ë¶„ì„ ë„êµ¬ ì„¤ì • ìƒìˆ˜"""

    # íŒŒì¼ í¬ê¸° ì œí•œ
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    MIN_INIT_FILE_SIZE = 50  # bytes

    # ì½”ë“œ ë¶„ì„
    CODE_PREVIEW_LENGTH = 2000
    HIGH_COMPLEXITY_THRESHOLD = 10
    CYCLOMATIC_COMPLEXITY_WARNING = 15

    # ì„±ëŠ¥
    MAX_WORKERS = 4
    CACHE_DIR = ".cache"

    # ì¸ì½”ë”©
    PRIMARY_ENCODING = 'utf-8'
    FALLBACK_ENCODING = 'latin-1'


# =============================================================================
# ë°ì´í„° í´ë˜ìŠ¤
# =============================================================================

@dataclass
class MethodInfo:
    """ë©”ì„œë“œ ì •ë³´"""
    name: str
    signature: str
    docstring: Optional[str]
    params: List[Dict[str, Any]] = field(default_factory=list)
    return_type: Optional[str] = None
    line_start: int = 0
    line_end: int = 0
    complexity: int = 1
    is_async: bool = False
    calls: List[Dict[str, str]] = field(default_factory=list)


@dataclass
class ClassInfo:
    """í´ë˜ìŠ¤ ì •ë³´"""
    name: str
    docstring: Optional[str]
    line_start: int
    line_end: int
    methods: List[MethodInfo] = field(default_factory=list)
    base_classes: List[str] = field(default_factory=list)


@dataclass
class ModuleStructure:
    """ëª¨ë“ˆ êµ¬ì¡°"""
    classes: List[ClassInfo] = field(default_factory=list)
    functions: List[MethodInfo] = field(default_factory=list)
    imports: List[Dict[str, Any]] = field(default_factory=list)
    constants: Dict[str, Any] = field(default_factory=dict)


# =============================================================================
# ë©”ì¸ ë¶„ì„ í´ë˜ìŠ¤
# =============================================================================

class ModuleAnalyzer:
    """ì½”ë“œë² ì´ìŠ¤ ìë™ ë¶„ì„ ë„êµ¬ (ê°œì„  ë²„ì „)"""

    def __init__(
        self,
        project_root: Path,
        use_ai: bool = True,
        use_cache: bool = True
    ):
        self.project_root = Path(project_root)
        self.use_ai = use_ai
        self.use_cache = use_cache
        self.output_dir = self.project_root / "docs" / "architecture"
        self.cache_dir = self.output_dir / Config.CACHE_DIR

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "modules").mkdir(exist_ok=True)
        (self.output_dir / "relationships").mkdir(exist_ok=True)

        if self.use_cache:
            self.cache_dir.mkdir(exist_ok=True)

        logger.info(f"ğŸ“ í”„ë¡œì íŠ¸: {self.project_root}")
        logger.info(f"ğŸ“„ ì¶œë ¥: {self.output_dir}")
        if self.use_cache:
            logger.info(f"ğŸ’¾ ìºì‹œ: í™œì„±í™”")

    async def analyze_project(
        self,
        target_dir: str = ".",
        exclude_patterns: Optional[List[str]] = None
    ):
        """ì „ì²´ í”„ë¡œì íŠ¸ ë¶„ì„"""

        if exclude_patterns is None:
            exclude_patterns = [
                "__pycache__", ".venv", "venv", ".env", ".git",
                "node_modules", "tests", "test_", "dist", "build"
            ]

        start_time = time.time()

        logger.info("\nğŸ” Step 1: Python íŒŒì¼ ìŠ¤ìº”...")
        python_files = self._find_python_files(
            self.project_root / target_dir,
            exclude_patterns
        )

        logger.info(f"   ë°œê²¬: {len(python_files)}ê°œ íŒŒì¼")

        if not python_files:
            logger.warning("âš ï¸  Python íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return

        # ëª¨ë“ˆë³„ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)
        logger.info("\nğŸ”— Step 2: ëª¨ë“ˆ êµ¬ì¡° ë¶„ì„...")

        all_modules = await self._analyze_all_files(python_files)

        # ì˜ì¡´ì„± ë¶„ì„
        logger.info("\nğŸ“Š Step 3: ì˜ì¡´ì„± ê´€ê³„ ë¶„ì„...")
        relationships = self._analyze_relationships(all_modules)

        # index.json ìƒì„±
        logger.info("\nğŸ“ Step 4: ë¬¸ì„œ ìƒì„±...")
        index = self._create_index(all_modules, relationships)

        # íŒŒì¼ ì €ì¥
        self._save_documentation(all_modules, relationships, index)

        elapsed = time.time() - start_time

        logger.info(f"\nâœ… ì™„ë£Œ!")
        logger.info(f"   ğŸ“Š ë¶„ì„: {len(all_modules)}ê°œ ëª¨ë“ˆ")
        logger.info(f"   â±ï¸  ì†Œìš”: {elapsed:.1f}ì´ˆ")
        logger.info(f"   ğŸ“‚ ì¶œë ¥: {self.output_dir}")
        logger.info(f"\nğŸ’¡ í™•ì¸: {self.output_dir / 'index.json'}")

    async def _analyze_all_files(
        self,
        python_files: List[Path]
    ) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  íŒŒì¼ì„ ë³‘ë ¬ë¡œ ë¶„ì„"""

        # ë¹„ë™ê¸° ì‘ì—… ìƒì„±
        tasks = [self.analyze_file(file_path) for file_path in python_files]

        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ìˆ˜ì§‘
        all_modules = {}
        for i, (file_path, result) in enumerate(zip(python_files, results), 1):
            rel_path = file_path.relative_to(self.project_root)

            if isinstance(result, Exception):
                logger.error(f"   [{i}/{len(python_files)}] {rel_path} âŒ {result}")
                continue

            status = "ğŸ’¾" if result.get('from_cache') else "âœ“"
            logger.info(f"   [{i}/{len(python_files)}] {rel_path} {status}")

            # ìºì‹œ í”Œë˜ê·¸ ì œê±°
            result.pop('from_cache', None)
            all_modules[result['module_id']] = result

        return all_modules

    def _find_python_files(
        self,
        directory: Path,
        exclude_patterns: List[str]
    ) -> List[Path]:
        """Python íŒŒì¼ ì°¾ê¸° (ë³´ì•ˆ ê°•í™”)"""

        python_files = []

        try:
            resolved_dir = directory.resolve()
        except (OSError, RuntimeError) as e:
            logger.error(f"âŒ ë””ë ‰í† ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return []

        for py_file in directory.rglob("*.py"):
            # Symlink ì²´í¬ (ë³´ì•ˆ)
            if py_file.is_symlink():
                logger.warning(f"   âš ï¸  Symlink ë¬´ì‹œ: {py_file}")
                continue

            # ê²½ë¡œ íƒìƒ‰ ê³µê²© ë°©ì§€
            try:
                py_file.resolve().relative_to(resolved_dir)
            except ValueError:
                logger.warning(f"   âš ï¸  ê²½ë¡œ ë²—ì–´ë‚¨: {py_file}")
                continue

            # ì œì™¸ íŒ¨í„´ ì²´í¬
            if any(pattern in str(py_file) for pattern in exclude_patterns):
                continue

            # íŒŒì¼ í¬ê¸° ì²´í¬
            try:
                file_size = py_file.stat().st_size

                if file_size > Config.MAX_FILE_SIZE:
                    logger.warning(
                        f"   âš ï¸  íŒŒì¼ ë„ˆë¬´ í¼: {py_file.name} "
                        f"({file_size / 1024 / 1024:.1f}MB)"
                    )
                    continue

                # __init__.pyëŠ” ë¹„ì–´ìˆìœ¼ë©´ ì œì™¸
                if py_file.name == "__init__.py":
                    if file_size < Config.MIN_INIT_FILE_SIZE:
                        continue

            except OSError as e:
                logger.warning(f"   âš ï¸  íŒŒì¼ ì ‘ê·¼ ì‹¤íŒ¨: {py_file} - {e}")
                continue

            python_files.append(py_file)

        return sorted(python_files)

    def _get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ìºì‹±ìš©)"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.sha256(f.read()).hexdigest()
        except Exception:
            return ""

    async def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„ (ìºì‹± ì§€ì›)"""

        # ëª¨ë“ˆ ID ìƒì„±
        rel_path = file_path.relative_to(self.project_root)
        module_id = str(rel_path.with_suffix('')).replace('/', '.').replace('\\', '.')

        # ìºì‹œ í™•ì¸
        if self.use_cache:
            cache_file = self.cache_dir / f"{module_id.replace('.', '_')}.json"
            if cache_file.exists():
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cached = json.load(f)

                    # íŒŒì¼ í•´ì‹œ ë¹„êµ
                    current_hash = self._get_file_hash(file_path)
                    if cached.get('file_hash') == current_hash:
                        cached['from_cache'] = True
                        return cached
                except Exception as e:
                    logger.debug(f"   ìºì‹œ ì½ê¸° ì‹¤íŒ¨: {e}")

        # ì½”ë“œ ì½ê¸° (ì¸ì½”ë”© ì²˜ë¦¬ ê°•í™”)
        code = self._read_file_safe(file_path)
        if code is None:
            return self._create_error_doc(module_id, str(rel_path), "Encoding error")

        # AST íŒŒì‹±
        try:
            tree = ast.parse(code)
        except SyntaxError as e:
            logger.debug(f"      âš ï¸  êµ¬ë¬¸ ì˜¤ë¥˜: {e}")
            return self._create_error_doc(module_id, str(rel_path), str(e))

        # êµ¬ì¡° ì¶”ì¶œ
        structure = self._extract_structure(tree, code)

        # AI ë¶„ì„ (ì˜µì…˜)
        if self.use_ai:
            try:
                ai_description = await self._ask_ai_about_module(
                    code, structure, module_id
                )
            except Exception as e:
                logger.debug(f"      âš ï¸  AI ë¶„ì„ ì‹¤íŒ¨: {e}")
                ai_description = self._create_default_description(structure)
        else:
            ai_description = self._create_default_description(structure)

        # ìµœì¢… ë¬¸ì„œ ìƒì„±
        module_doc = self._create_module_doc(
            module_id=module_id,
            file_path=str(rel_path),
            structure=structure,
            ai_description=ai_description,
            code=code
        )

        # íŒŒì¼ í•´ì‹œ ì¶”ê°€
        module_doc['file_hash'] = self._get_file_hash(file_path)

        # ìºì‹œ ì €ì¥
        if self.use_cache:
            try:
                cache_file = self.cache_dir / f"{module_id.replace('.', '_')}.json"
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump(module_doc, f, indent=2, ensure_ascii=False)
            except Exception as e:
                logger.debug(f"   ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

        return module_doc

    def _read_file_safe(self, file_path: Path) -> Optional[str]:
        """ì•ˆì „í•œ íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ì²˜ë¦¬)"""

        # UTF-8 ì‹œë„
        try:
            with open(file_path, 'r', encoding=Config.PRIMARY_ENCODING) as f:
                return f.read()
        except UnicodeDecodeError:
            pass

        # Fallback ì¸ì½”ë”© ì‹œë„
        try:
            with open(file_path, 'r', encoding=Config.FALLBACK_ENCODING) as f:
                return f.read()
        except Exception as e:
            logger.warning(f"      âš ï¸  ì¸ì½”ë”© ì˜¤ë¥˜: {file_path.name} - {e}")
            return None

    def _extract_structure(self, tree: ast.AST, code: str) -> ModuleStructure:
        """ASTì—ì„œ êµ¬ì¡° ì¶”ì¶œ"""

        classes = []
        functions = []
        imports = []
        constants = {}

        code_lines = code.split('\n')

        for node in tree.body:
            if isinstance(node, ast.ClassDef):
                class_info = self._extract_class(node, code_lines)
                classes.append(class_info)

            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                func_info = self._extract_method(node, code_lines)
                functions.append(func_info)

            elif isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append({
                        'module': alias.name,
                        'alias': alias.asname,
                        'type': 'import'
                    })

            elif isinstance(node, ast.ImportFrom):
                imports.append({
                    'module': node.module or '',
                    'items': [alias.name for alias in node.names],
                    'type': 'from'
                })

            elif isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        if target.id.isupper():
                            try:
                                value = ast.literal_eval(node.value)
                                constants[target.id] = value
                            except (ValueError, SyntaxError):
                                constants[target.id] = "<complex>"

        return ModuleStructure(
            classes=classes,
            functions=functions,
            imports=imports,
            constants=constants
        )

    def _extract_class(self, node: ast.ClassDef, code_lines: List[str]) -> ClassInfo:
        """í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ"""

        methods = []

        for item in node.body:
            if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                method_info = self._extract_method(item, code_lines)
                methods.append(method_info)

        base_classes = []
        for base in node.bases:
            if isinstance(base, ast.Name):
                base_classes.append(base.id)
            elif isinstance(base, ast.Attribute):
                try:
                    base_classes.append(ast.unparse(base))
                except Exception:
                    base_classes.append("<complex>")

        return ClassInfo(
            name=node.name,
            docstring=ast.get_docstring(node),
            line_start=node.lineno,
            line_end=node.end_lineno or node.lineno,
            methods=methods,
            base_classes=base_classes
        )

    def _extract_method(
        self,
        node: ast.FunctionDef,
        code_lines: List[str]
    ) -> MethodInfo:
        """ë©”ì„œë“œ/í•¨ìˆ˜ ì •ë³´ ì¶”ì¶œ (ìµœì í™”: í•œ ë²ˆì˜ walk)"""

        # íŒŒë¼ë¯¸í„°
        params = []
        for arg in node.args.args:
            param = {'name': arg.arg}
            if arg.annotation:
                try:
                    param['type'] = ast.unparse(arg.annotation)
                except Exception:
                    param['type'] = "<complex>"
            params.append(param)

        # ë¦¬í„´ íƒ€ì…
        return_type = None
        if node.returns:
            try:
                return_type = ast.unparse(node.returns)
            except Exception:
                return_type = "<complex>"

        # ì‹œê·¸ë‹ˆì²˜ ìƒì„±
        param_strs = []
        for p in params:
            if 'type' in p:
                param_strs.append(f"{p['name']}: {p['type']}")
            else:
                param_strs.append(p['name'])

        sig = f"{'async ' if isinstance(node, ast.AsyncFunctionDef) else ''}def {node.name}({', '.join(param_strs)})"
        if return_type:
            sig += f" -> {return_type}"

        # í•œ ë²ˆì˜ walkë¡œ callsì™€ complexity ì¶”ì¶œ
        calls, complexity = self._extract_calls_and_complexity(node)

        return MethodInfo(
            name=node.name,
            signature=sig,
            docstring=ast.get_docstring(node),
            params=params,
            return_type=return_type,
            line_start=node.lineno,
            line_end=node.end_lineno or node.lineno,
            complexity=complexity,
            is_async=isinstance(node, ast.AsyncFunctionDef),
            calls=calls
        )

    def _extract_calls_and_complexity(
        self,
        node: ast.FunctionDef
    ) -> Tuple[List[Dict[str, str]], int]:
        """í•¨ìˆ˜ ë‚´ë¶€ í˜¸ì¶œ ë° ë³µì¡ë„ë¥¼ í•œ ë²ˆì˜ ìˆœíšŒë¡œ ê³„ì‚° (ìµœì í™”)"""

        calls = []
        seen_calls: Set[str] = set()
        complexity = 1

        for n in ast.walk(node):
            # Complexity ê³„ì‚°
            if isinstance(n, (ast.If, ast.For, ast.While, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(n, ast.BoolOp):
                complexity += len(n.values) - 1

            # Calls ì¶”ì¶œ
            if isinstance(n, ast.Call):
                call_info = None

                if isinstance(n.func, ast.Attribute):
                    if isinstance(n.func.value, ast.Name) and n.func.value.id == 'self':
                        call_info = {'type': 'method', 'name': n.func.attr}
                    else:
                        try:
                            call_info = {'type': 'external', 'name': ast.unparse(n.func)}
                        except Exception:
                            call_info = {'type': 'external', 'name': '<unparseable>'}

                elif isinstance(n.func, ast.Name):
                    call_info = {'type': 'function', 'name': n.func.id}

                if call_info:
                    key = f"{call_info['type']}:{call_info['name']}"
                    if key not in seen_calls:
                        calls.append(call_info)
                        seen_calls.add(key)

        return calls, complexity

    async def _ask_ai_about_module(
        self,
        code: str,
        structure: ModuleStructure,
        module_id: str
    ) -> Dict[str, Any]:
        """AIì—ê²Œ ëª¨ë“ˆ ì—­í•  ë¬¼ì–´ë³´ê¸°"""

        code_preview = code[:Config.CODE_PREVIEW_LENGTH] if len(code) > Config.CODE_PREVIEW_LENGTH else code

        class_names = [c.name for c in structure.classes]
        method_summary = []
        for cls in structure.classes:
            for method in cls.methods[:5]:
                method_summary.append(f"{cls.name}.{method.name}()")

        import_modules = [imp['module'] for imp in structure.imports[:10]]

        prompt = f"""ë‹¤ìŒ Python ëª¨ë“ˆì„ ë¶„ì„í•˜ê³  ì—­í• ì„ ì„¤ëª…í•˜ì„¸ìš”.

**ëª¨ë“ˆ**: {module_id}
**í´ë˜ìŠ¤**: {class_names}
**ì£¼ìš” ë©”ì„œë“œ**: {method_summary}
**Import**: {import_modules}

**ì½”ë“œ ì¼ë¶€**:
```python
{code_preview}
```

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´):
{{
  "one_liner": "í•œ ì¤„ ìš”ì•½ (50ì ì´ë‚´)",
  "description": "ìƒì„¸ ì„¤ëª… (100-200ì)",
  "responsibilities": ["ì±…ì„1", "ì±…ì„2", "ì±…ì„3"],
  "key_patterns": ["íŒ¨í„´1", "íŒ¨í„´2"],
  "complexity_level": "low|medium|high",
  "importance": "low|normal|high|critical"
}}

ì¤‘ìš”: JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” êµ¬í˜„ í•„ìš”)
        raise Exception("AI API not configured - use --no-ai flag")

    def _create_default_description(self, structure: ModuleStructure) -> Dict[str, Any]:
        """AI ì—†ì´ ê¸°ë³¸ ì„¤ëª… ìƒì„±"""

        if structure.classes:
            class_names = [c.name for c in structure.classes]
            one_liner = f"{', '.join(class_names[:2])} í´ë˜ìŠ¤ í¬í•¨"

            responsibilities = []
            for cls in structure.classes:
                if cls.methods:
                    responsibilities.append(f"{cls.name}: {len(cls.methods)}ê°œ ë©”ì„œë“œ")
        else:
            one_liner = f"{len(structure.functions)}ê°œ í•¨ìˆ˜ í¬í•¨"
            responsibilities = [f.name for f in structure.functions[:3]]

        total_complexity = sum(m.complexity for c in structure.classes for m in c.methods)
        total_complexity += sum(f.complexity for f in structure.functions)

        if total_complexity > 50:
            complexity_level = "high"
        elif total_complexity > 20:
            complexity_level = "medium"
        else:
            complexity_level = "low"

        return {
            "one_liner": one_liner,
            "description": f"ì´ ëª¨ë“ˆì€ {len(structure.classes)}ê°œ í´ë˜ìŠ¤ì™€ {len(structure.functions)}ê°œ í•¨ìˆ˜ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.",
            "responsibilities": responsibilities[:5],
            "key_patterns": [],
            "complexity_level": complexity_level,
            "importance": "normal"
        }

    def _create_module_doc(
        self,
        module_id: str,
        file_path: str,
        structure: ModuleStructure,
        ai_description: Dict[str, Any],
        code: str
    ) -> Dict[str, Any]:
        """ìµœì¢… ëª¨ë“ˆ ë¬¸ì„œ ìƒì„±"""

        classes_dict = []
        for cls in structure.classes:
            cls_dict = {
                'name': cls.name,
                'docstring': cls.docstring,
                'line_start': cls.line_start,
                'line_end': cls.line_end,
                'base_classes': cls.base_classes,
                'methods': []
            }

            for method in cls.methods:
                method_dict = {
                    'name': method.name,
                    'signature': method.signature,
                    'docstring': method.docstring,
                    'params': method.params,
                    'return_type': method.return_type,
                    'line_start': method.line_start,
                    'line_end': method.line_end,
                    'complexity': method.complexity,
                    'is_async': method.is_async,
                    'calls': method.calls
                }
                cls_dict['methods'].append(method_dict)

            classes_dict.append(cls_dict)

        functions_dict = []
        for func in structure.functions:
            functions_dict.append({
                'name': func.name,
                'signature': func.signature,
                'docstring': func.docstring,
                'params': func.params,
                'return_type': func.return_type,
                'line_start': func.line_start,
                'line_end': func.line_end,
                'complexity': func.complexity,
                'is_async': func.is_async,
                'calls': func.calls
            })

        doc = {
            'module_id': module_id,
            'file_path': file_path,
            'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),

            'summary': {
                'one_liner': ai_description['one_liner'],
                'description': ai_description['description'],
                'responsibilities': ai_description['responsibilities'],
                'complexity_level': ai_description['complexity_level'],
                'importance': ai_description['importance']
            },

            'structure': {
                'classes': classes_dict,
                'functions': functions_dict,
                'imports': structure.imports,
                'constants': structure.constants
            },

            'metrics': {
                'total_lines': len(code.split('\n')),
                'num_classes': len(structure.classes),
                'num_functions': len(structure.functions),
                'num_methods': sum(len(c.methods) for c in structure.classes),
                'total_complexity': sum(
                    m.complexity for c in structure.classes for m in c.methods
                ) + sum(f.complexity for f in structure.functions)
            }
        }

        # ë³µì¡ë„ ê²½ê³ 
        high_complexity_methods = []
        for cls in structure.classes:
            for method in cls.methods:
                if method.complexity > Config.HIGH_COMPLEXITY_THRESHOLD:
                    high_complexity_methods.append({
                        'class': cls.name,
                        'method': method.name,
                        'complexity': method.complexity,
                        'lines': [method.line_start, method.line_end]
                    })

        for func in structure.functions:
            if func.complexity > Config.HIGH_COMPLEXITY_THRESHOLD:
                high_complexity_methods.append({
                    'function': func.name,
                    'complexity': func.complexity,
                    'lines': [func.line_start, func.line_end]
                })

        if high_complexity_methods:
            doc['warnings'] = {'high_complexity': high_complexity_methods}

        return doc

    def _create_error_doc(
        self,
        module_id: str,
        file_path: str,
        error: str
    ) -> Dict[str, Any]:
        """ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ë¬¸ì„œ"""

        return {
            'module_id': module_id,
            'file_path': file_path,
            'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
            'error': error,
            'summary': {
                'one_liner': 'ë¶„ì„ ì‹¤íŒ¨',
                'description': f'êµ¬ë¬¸ ì˜¤ë¥˜: {error}',
                'responsibilities': [],
                'complexity_level': 'unknown',
                'importance': 'unknown'
            },
            'structure': {
                'classes': [],
                'functions': [],
                'imports': [],
                'constants': {}
            },
            'metrics': {
                'total_lines': 0,
                'num_classes': 0,
                'num_functions': 0,
                'num_methods': 0,
                'total_complexity': 0
            }
        }

    def _analyze_relationships(
        self,
        all_modules: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ëª¨ë“ˆ ê°„ ê´€ê³„ ë¶„ì„"""

        import_graph = {}
        call_graph = {}

        for module_id, module_doc in all_modules.items():
            imports = []
            for imp in module_doc['structure']['imports']:
                imp_module = imp['module']
                if self._is_internal_module(imp_module, all_modules):
                    imports.append(imp_module)

            import_graph[module_id] = imports

            calls = []
            for cls in module_doc['structure']['classes']:
                for method in cls['methods']:
                    for call in method['calls']:
                        if call['type'] == 'method':
                            calls.append({
                                'from': f"{module_id}.{cls['name']}.{method['name']}",
                                'to': call['name'],
                                'type': 'method_call'
                            })

            call_graph[module_id] = calls

        # ìˆœí™˜ ì˜ì¡´ì„± ì°¾ê¸° (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)
        cycles = self._find_cycles(import_graph)

        return {
            'import_graph': import_graph,
            'call_graph': call_graph,
            'cycles': cycles,
            'statistics': {
                'total_imports': sum(len(v) for v in import_graph.values()),
                'total_calls': sum(len(v) for v in call_graph.values()),
                'num_cycles': len(cycles)
            }
        }

    def _is_internal_module(
        self,
        module_name: str,
        all_modules: Dict[str, Dict[str, Any]]
    ) -> bool:
        """ë‚´ë¶€ ëª¨ë“ˆì¸ì§€ í™•ì¸ (ê°œì„ ëœ ë¡œì§)"""

        # ì§ì ‘ ë§¤ì¹­
        if module_name in all_modules:
            return True

        # ë¶€ëª¨/ìì‹ ê´€ê³„ ì²´í¬
        mod_parts = module_name.split('.')

        for mod_id in all_modules.keys():
            mod_id_parts = mod_id.split('.')

            # ì •í™•í•œ prefix ë§¤ì¹­
            min_len = min(len(mod_parts), len(mod_id_parts))
            if mod_parts[:min_len] == mod_id_parts[:min_len]:
                return True

        return False

    def _find_cycles(self, graph: Dict[str, List[str]]) -> List[List[str]]:
        """ìˆœí™˜ ì˜ì¡´ì„± ì°¾ê¸° (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)"""

        cycles = []
        visited: Set[str] = set()
        rec_stack: Set[str] = set()

        def dfs(node: str, path: List[str]) -> None:
            if node in rec_stack:
                # ìˆœí™˜ ë°œê²¬
                try:
                    cycle_start = path.index(node)
                    cycle = path[cycle_start:]

                    # ì •ê·œí™”ëœ í˜•íƒœë¡œ ì €ì¥ (ì¤‘ë³µ ì œê±°)
                    # ìˆœí™˜ì˜ ìµœì†Œ ë…¸ë“œë¶€í„° ì‹œì‘í•˜ë„ë¡ ì •ë ¬
                    min_idx = cycle.index(min(cycle))
                    normalized = cycle[min_idx:] + cycle[:min_idx]

                    # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    is_duplicate = False
                    for existing_cycle in cycles:
                        if len(existing_cycle) == len(normalized):
                            # íšŒì „ëœ í˜•íƒœë„ ì²´í¬
                            for i in range(len(normalized)):
                                rotated = normalized[i:] + normalized[:i]
                                if existing_cycle == rotated:
                                    is_duplicate = True
                                    break
                        if is_duplicate:
                            break

                    if not is_duplicate:
                        cycles.append(normalized)
                except ValueError:
                    pass
                return

            if node in visited:
                return

            visited.add(node)
            rec_stack.add(node)

            for neighbor in graph.get(node, []):
                dfs(neighbor, path + [neighbor])

            rec_stack.remove(node)

        for node in graph:
            if node not in visited:
                dfs(node, [node])

        return cycles

    def _create_index(
        self,
        all_modules: Dict[str, Dict[str, Any]],
        relationships: Dict[str, Any]
    ) -> Dict[str, Any]:
        """index.json ìƒì„±"""

        layers = self._categorize_layers(all_modules)

        modules_summary = []
        for module_id, module_doc in all_modules.items():
            modules_summary.append({
                'id': module_id,
                'role': module_doc['summary']['one_liner'],
                'complexity': module_doc['summary']['complexity_level'],
                'importance': module_doc['summary']['importance'],
                'loc': module_doc['metrics']['total_lines'],
                'detail_file': f"modules/{module_id.replace('.', '_')}.json"
            })

        importance_order = {'critical': 0, 'high': 1, 'normal': 2, 'low': 3}
        modules_summary.sort(
            key=lambda m: (importance_order.get(m['importance'], 9), -m['loc'])
        )

        index = {
            'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
            'project': self.project_root.name,
            'total_modules': len(all_modules),
            'total_loc': sum(m['metrics']['total_lines'] for m in all_modules.values()),

            'layers': layers,
            'modules': modules_summary,

            'statistics': {
                'total_classes': sum(m['metrics']['num_classes'] for m in all_modules.values()),
                'total_functions': sum(m['metrics']['num_functions'] for m in all_modules.values()),
                'total_imports': relationships['statistics']['total_imports'],
                'circular_dependencies': len(relationships['cycles'])
            },

            'warnings': []
        }

        if relationships['cycles']:
            index['warnings'].append({
                'type': 'circular_dependency',
                'count': len(relationships['cycles']),
                'severity': 'medium',
                'cycles': relationships['cycles']
            })

        high_complexity_modules = [
            m['module_id'] for m in all_modules.values()
            if m['summary']['complexity_level'] == 'high'
        ]
        if high_complexity_modules:
            index['warnings'].append({
                'type': 'high_complexity',
                'modules': high_complexity_modules,
                'severity': 'low'
            })

        return index

    def _categorize_layers(
        self,
        all_modules: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ëª¨ë“ˆì„ ë ˆì´ì–´ë³„ë¡œ ë¶„ë¥˜"""

        layers = {}

        for module_id in all_modules.keys():
            parts = module_id.split('.')

            if len(parts) >= 2:
                layer_name = parts[0]

                if layer_name not in layers:
                    layers[layer_name] = {
                        'modules': [],
                        'count': 0
                    }

                layers[layer_name]['modules'].append(module_id)
                layers[layer_name]['count'] += 1

        return layers

    def _save_documentation(
        self,
        all_modules: Dict[str, Dict[str, Any]],
        relationships: Dict[str, Any],
        index: Dict[str, Any]
    ):
        """ë¬¸ì„œ íŒŒì¼ ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬)"""

        def save_json(path: Path, data: Dict[str, Any]) -> None:
            """JSON íŒŒì¼ ì €ì¥ í—¬í¼"""
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
            futures = []

            # 1. index.json
            index_path = self.output_dir / "index.json"
            futures.append(executor.submit(save_json, index_path, index))

            # 2. ê° ëª¨ë“ˆ ìƒì„¸
            for module_id, module_doc in all_modules.items():
                filename = f"{module_id.replace('.', '_')}.json"
                module_path = self.output_dir / "modules" / filename
                futures.append(executor.submit(save_json, module_path, module_doc))

            # 3. ê´€ê³„ ì •ë³´
            rel_path = self.output_dir / "relationships" / "dependencies.json"
            futures.append(executor.submit(save_json, rel_path, relationships))

            # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            for future in futures:
                future.result()

        logger.info(f"   âœ“ index.json")
        logger.info(f"   âœ“ {len(all_modules)}ê°œ ëª¨ë“ˆ ë¬¸ì„œ")
        logger.info(f"   âœ“ dependencies.json")


# =============================================================================
# CLI ì§„ì…ì 
# =============================================================================

async def main():
    """CLI ì§„ì…ì """

    import argparse

    parser = argparse.ArgumentParser(
        description='Python ì½”ë“œë² ì´ìŠ¤ ìë™ ë¶„ì„ ë„êµ¬ (ê°œì„  ë²„ì „)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python module_analyzer.py /path/to/project
  python module_analyzer.py /path/to/project --target src
  python module_analyzer.py /path/to/project --no-ai --no-cache
  python module_analyzer.py /path/to/project --exclude tests docs
        """
    )

    parser.add_argument(
        'project_root',
        help='í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬'
    )

    parser.add_argument(
        '--target',
        default='.',
        help='ë¶„ì„í•  í•˜ìœ„ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ì „ì²´)'
    )

    parser.add_argument(
        '--no-ai',
        action='store_true',
        help='AI ë¶„ì„ ë¹„í™œì„±í™” (êµ¬ì¡°ë§Œ ì¶”ì¶œ)'
    )

    parser.add_argument(
        '--no-cache',
        action='store_true',
        help='ìºì‹± ë¹„í™œì„±í™”'
    )

    parser.add_argument(
        '--exclude',
        nargs='+',
        help='ì œì™¸í•  íŒ¨í„´ (ì˜ˆ: tests __pycache__)'
    )

    args = parser.parse_args()

    # ë¶„ì„ ì‹¤í–‰
    analyzer = ModuleAnalyzer(
        project_root=args.project_root,
        use_ai=not args.no_ai,
        use_cache=not args.no_cache
    )

    await analyzer.analyze_project(
        target_dir=args.target,
        exclude_patterns=args.exclude
    )


if __name__ == '__main__':
    asyncio.run(main())
