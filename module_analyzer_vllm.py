"""
Module Analyzer with Local LLM (vLLM + Llama)
ì½”ë“œë² ì´ìŠ¤ ìë™ ë¶„ì„ ë° ë¬¸ì„œí™” ë„êµ¬ - ë¡œì»¬ LLM ë²„ì „

ì‚¬ìš©ë²•:
    # vLLM ì„œë²„ ë¨¼ì € ì‹œì‘ (ë³„ë„ í„°ë¯¸ë„):
    python -m vllm.entrypoints.openai.api_server \
        --model meta-llama/Llama-3.2-3B-Instruct \
        --port 8000

    # ë¶„ì„ ì‹¤í–‰:
    python module_analyzer_vllm.py /path/to/project
    python module_analyzer_vllm.py /path/to/project --llm-url http://localhost:8000
    python module_analyzer_vllm.py /path/to/project --no-ai
"""

import ast
import json
import asyncio
import time
import hashlib
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
import logging

# HTTP í´ë¼ì´ì–¸íŠ¸
try:
    import httpx
    HAS_HTTPX = True
except ImportError:
    HAS_HTTPX = False
    logging.warning("âš ï¸  httpxê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. AI ê¸°ëŠ¥ ë¹„í™œì„±í™”. pip install httpx")

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


# =============================================================================
# LLM í´ë¼ì´ì–¸íŠ¸
# =============================================================================

class LLMClient:
    """ë¡œì»¬ LLM í´ë¼ì´ì–¸íŠ¸ (vLLM + Llama)"""

    def __init__(
        self,
        base_url: str = "http://localhost:8000",
        model: str = "meta-llama/Llama-3.2-3B-Instruct",
        timeout: float = 60.0
    ):
        if not HAS_HTTPX:
            raise ImportError("httpx í•„ìš”. pip install httpx")

        self.base_url = base_url.rstrip('/')
        self.model = model
        self.timeout = timeout

        logger.info(f"ğŸ¤– LLM: {model}")
        logger.info(f"ğŸ”— ì„œë²„: {base_url}")

    async def test_connection(self) -> bool:
        """ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/health")
                return response.status_code == 200
        except Exception as e:
            logger.error(f"âŒ LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    async def complete(
        self,
        prompt: str,
        max_tokens: int = 600,
        temperature: float = 0.1,
        stop: Optional[List[str]] = None
    ) -> str:
        """LLM ì™„ì„± ìš”ì²­"""

        if stop is None:
            stop = ["```\n", "\n\n\n\n", "<|eot_id|>"]

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                # OpenAI-compatible API
                response = await client.post(
                    f"{self.base_url}/v1/completions",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "max_tokens": max_tokens,
                        "temperature": temperature,
                        "top_p": 0.95,
                        "stop": stop
                    }
                )

                response.raise_for_status()
                result = response.json()

                if 'choices' in result and len(result['choices']) > 0:
                    return result['choices'][0]['text'].strip()
                else:
                    raise ValueError(f"Invalid response format: {result}")

            except httpx.TimeoutException:
                raise Exception("LLM ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
            except httpx.HTTPStatusError as e:
                raise Exception(f"LLM ì„œë²„ ì˜¤ë¥˜: {e.response.status_code}")
            except Exception as e:
                raise Exception(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")


# =============================================================================
# ìƒìˆ˜ ì •ì˜
# =============================================================================

class Config:
    """ë¶„ì„ ë„êµ¬ ì„¤ì • ìƒìˆ˜"""

    # íŒŒì¼ í¬ê¸° ì œí•œ
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    MIN_INIT_FILE_SIZE = 50  # bytes

    # ì½”ë“œ ë¶„ì„
    CODE_PREVIEW_LENGTH = 1500  # LLM ì»¨í…ìŠ¤íŠ¸ ì ˆì•½
    HIGH_COMPLEXITY_THRESHOLD = 10
    CYCLOMATIC_COMPLEXITY_WARNING = 15

    # ì„±ëŠ¥
    MAX_WORKERS = 4
    CACHE_DIR = ".cache"

    # ì¸ì½”ë”©
    PRIMARY_ENCODING = 'utf-8'
    FALLBACK_ENCODING = 'latin-1'

    # LLM
    LLM_RETRY_COUNT = 3
    LLM_TIMEOUT = 60.0
    LLM_MAX_TOKENS = 600


# =============================================================================
# ë°ì´í„° í´ë˜ìŠ¤
# =============================================================================

@dataclass
class MethodInfo:
    """ë©”ì„œë“œ ì •ë³´"""
    name: str
    signature: str
    docstring: Optional[str]
    params: List[Dict[str, Any]] = field(default_factory=list)
    return_type: Optional[str] = None
    line_start: int = 0
    line_end: int = 0
    complexity: int = 1
    is_async: bool = False
    calls: List[Dict[str, str]] = field(default_factory=list)


@dataclass
class ClassInfo:
    """í´ë˜ìŠ¤ ì •ë³´"""
    name: str
    docstring: Optional[str]
    line_start: int
    line_end: int
    methods: List[MethodInfo] = field(default_factory=list)
    base_classes: List[str] = field(default_factory=list)


@dataclass
class ModuleStructure:
    """ëª¨ë“ˆ êµ¬ì¡°"""
    classes: List[ClassInfo] = field(default_factory=list)
    functions: List[MethodInfo] = field(default_factory=list)
    imports: List[Dict[str, Any]] = field(default_factory=list)
    constants: Dict[str, Any] = field(default_factory=dict)


# =============================================================================
# ë©”ì¸ ë¶„ì„ í´ë˜ìŠ¤
# =============================================================================

class ModuleAnalyzer:
    """ì½”ë“œë² ì´ìŠ¤ ìë™ ë¶„ì„ ë„êµ¬ (vLLM í†µí•© ë²„ì „)"""

    def __init__(
        self,
        project_root: Path,
        use_ai: bool = True,
        use_cache: bool = True,
        llm_url: str = "http://localhost:8000",
        llm_model: str = "meta-llama/Llama-3.2-3B-Instruct"
    ):
        self.project_root = Path(project_root)
        self.use_ai = use_ai
        self.use_cache = use_cache
        self.output_dir = self.project_root / "docs" / "architecture"
        self.cache_dir = self.output_dir / Config.CACHE_DIR

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "modules").mkdir(exist_ok=True)
        (self.output_dir / "relationships").mkdir(exist_ok=True)

        if self.use_cache:
            self.cache_dir.mkdir(exist_ok=True)

        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.llm = None
        if self.use_ai and HAS_HTTPX:
            try:
                self.llm = LLMClient(base_url=llm_url, model=llm_model)
            except Exception as e:
                logger.warning(f"âš ï¸  LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_ai = False

        logger.info(f"ğŸ“ í”„ë¡œì íŠ¸: {self.project_root}")
        logger.info(f"ğŸ“„ ì¶œë ¥: {self.output_dir}")
        if self.use_cache:
            logger.info(f"ğŸ’¾ ìºì‹œ: í™œì„±í™”")
        if not self.use_ai:
            logger.info(f"ğŸš« AI: ë¹„í™œì„±í™” (êµ¬ì¡°ë§Œ ë¶„ì„)")

    async def analyze_project(
        self,
        target_dir: str = ".",
        exclude_patterns: Optional[List[str]] = None
    ):
        """ì „ì²´ í”„ë¡œì íŠ¸ ë¶„ì„"""

        if exclude_patterns is None:
            exclude_patterns = [
                "__pycache__", ".venv", "venv", ".env", ".git",
                "node_modules", "tests", "test_", "dist", "build"
            ]

        # LLM ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
        if self.use_ai and self.llm:
            logger.info("\nğŸ”Œ LLM ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸...")
            if await self.llm.test_connection():
                logger.info("   âœ“ ì—°ê²° ì„±ê³µ")
            else:
                logger.warning("   âš ï¸  ì—°ê²° ì‹¤íŒ¨ - AI ë¶„ì„ ë¹„í™œì„±í™”")
                self.use_ai = False

        start_time = time.time()

        logger.info("\nğŸ” Step 1: Python íŒŒì¼ ìŠ¤ìº”...")
        python_files = self._find_python_files(
            self.project_root / target_dir,
            exclude_patterns
        )

        logger.info(f"   ë°œê²¬: {len(python_files)}ê°œ íŒŒì¼")

        if not python_files:
            logger.warning("âš ï¸  Python íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return

        # ëª¨ë“ˆë³„ ë¶„ì„
        logger.info("\nğŸ”— Step 2: ëª¨ë“ˆ êµ¬ì¡° ë¶„ì„...")
        all_modules = await self._analyze_all_files(python_files)

        # ì˜ì¡´ì„± ë¶„ì„
        logger.info("\nğŸ“Š Step 3: ì˜ì¡´ì„± ê´€ê³„ ë¶„ì„...")
        relationships = self._analyze_relationships(all_modules)

        # index.json ìƒì„±
        logger.info("\nğŸ“ Step 4: ë¬¸ì„œ ìƒì„±...")
        index = self._create_index(all_modules, relationships)

        # íŒŒì¼ ì €ì¥
        self._save_documentation(all_modules, relationships, index)

        elapsed = time.time() - start_time

        logger.info(f"\nâœ… ì™„ë£Œ!")
        logger.info(f"   ğŸ“Š ë¶„ì„: {len(all_modules)}ê°œ ëª¨ë“ˆ")
        logger.info(f"   â±ï¸  ì†Œìš”: {elapsed:.1f}ì´ˆ")
        if self.use_ai and self.llm:
            logger.info(f"   ğŸ¤– AI: í™œì„±í™”")
        logger.info(f"   ğŸ“‚ ì¶œë ¥: {self.output_dir}")
        logger.info(f"\nğŸ’¡ í™•ì¸: {self.output_dir / 'index.json'}")

    async def _analyze_all_files(
        self,
        python_files: List[Path]
    ) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  íŒŒì¼ì„ ë³‘ë ¬ë¡œ ë¶„ì„"""

        tasks = [self.analyze_file(file_path) for file_path in python_files]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_modules = {}
        for i, (file_path, result) in enumerate(zip(python_files, results), 1):
            rel_path = file_path.relative_to(self.project_root)

            if isinstance(result, Exception):
                logger.error(f"   [{i}/{len(python_files)}] {rel_path} âŒ {result}")
                continue

            status = "ğŸ’¾" if result.get('from_cache') else ("ğŸ¤–" if self.use_ai else "âœ“")
            logger.info(f"   [{i}/{len(python_files)}] {rel_path} {status}")

            result.pop('from_cache', None)
            all_modules[result['module_id']] = result

        return all_modules

    def _find_python_files(
        self,
        directory: Path,
        exclude_patterns: List[str]
    ) -> List[Path]:
        """Python íŒŒì¼ ì°¾ê¸°"""

        python_files = []

        try:
            resolved_dir = directory.resolve()
        except (OSError, RuntimeError) as e:
            logger.error(f"âŒ ë””ë ‰í† ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return []

        for py_file in directory.rglob("*.py"):
            # Symlink ì²´í¬
            if py_file.is_symlink():
                continue

            # ê²½ë¡œ íƒìƒ‰ ê³µê²© ë°©ì§€
            try:
                py_file.resolve().relative_to(resolved_dir)
            except ValueError:
                continue

            # ì œì™¸ íŒ¨í„´ ì²´í¬
            if any(pattern in str(py_file) for pattern in exclude_patterns):
                continue

            # íŒŒì¼ í¬ê¸° ì²´í¬
            try:
                file_size = py_file.stat().st_size

                if file_size > Config.MAX_FILE_SIZE:
                    logger.warning(f"   âš ï¸  íŒŒì¼ ë„ˆë¬´ í¼: {py_file.name}")
                    continue

                # __init__.pyëŠ” ë¹„ì–´ìˆìœ¼ë©´ ì œì™¸
                if py_file.name == "__init__.py":
                    if file_size < Config.MIN_INIT_FILE_SIZE:
                        continue

            except OSError:
                continue

            python_files.append(py_file)

        return sorted(python_files)

    def _get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.sha256(f.read()).hexdigest()
        except Exception:
            return ""

    async def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""

        rel_path = file_path.relative_to(self.project_root)
        module_id = str(rel_path.with_suffix('')).replace('/', '.').replace('\\', '.')

        # ìºì‹œ í™•ì¸
        if self.use_cache:
            cache_file = self.cache_dir / f"{module_id.replace('.', '_')}.json"
            if cache_file.exists():
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cached = json.load(f)

                    current_hash = self._get_file_hash(file_path)
                    if cached.get('file_hash') == current_hash:
                        cached['from_cache'] = True
                        return cached
                except Exception:
                    pass

        # ì½”ë“œ ì½ê¸°
        code = self._read_file_safe(file_path)
        if code is None:
            return self._create_error_doc(module_id, str(rel_path), "Encoding error")

        # AST íŒŒì‹±
        try:
            tree = ast.parse(code)
        except SyntaxError as e:
            return self._create_error_doc(module_id, str(rel_path), str(e))

        # êµ¬ì¡° ì¶”ì¶œ
        structure = self._extract_structure(tree, code)

        # AI ë¶„ì„
        if self.use_ai and self.llm:
            try:
                ai_description = await self._ask_ai_about_module(
                    code, structure, module_id
                )
            except Exception as e:
                logger.debug(f"      AI ì‹¤íŒ¨: {e}")
                ai_description = self._create_default_description(structure)
        else:
            ai_description = self._create_default_description(structure)

        # ìµœì¢… ë¬¸ì„œ ìƒì„±
        module_doc = self._create_module_doc(
            module_id=module_id,
            file_path=str(rel_path),
            structure=structure,
            ai_description=ai_description,
            code=code
        )

        module_doc['file_hash'] = self._get_file_hash(file_path)

        # ìºì‹œ ì €ì¥
        if self.use_cache:
            try:
                cache_file = self.cache_dir / f"{module_id.replace('.', '_')}.json"
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump(module_doc, f, indent=2, ensure_ascii=False)
            except Exception:
                pass

        return module_doc

    def _read_file_safe(self, file_path: Path) -> Optional[str]:
        """ì•ˆì „í•œ íŒŒì¼ ì½ê¸°"""
        try:
            with open(file_path, 'r', encoding=Config.PRIMARY_ENCODING) as f:
                return f.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding=Config.FALLBACK_ENCODING) as f:
                    return f.read()
            except Exception:
                return None

    def _extract_structure(self, tree: ast.AST, code: str) -> ModuleStructure:
        """ASTì—ì„œ êµ¬ì¡° ì¶”ì¶œ"""

        classes = []
        functions = []
        imports = []
        constants = {}

        for node in tree.body:
            if isinstance(node, ast.ClassDef):
                classes.append(self._extract_class(node))

            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                functions.append(self._extract_method(node))

            elif isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append({
                        'module': alias.name,
                        'alias': alias.asname,
                        'type': 'import'
                    })

            elif isinstance(node, ast.ImportFrom):
                imports.append({
                    'module': node.module or '',
                    'items': [alias.name for alias in node.names],
                    'type': 'from'
                })

            elif isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and target.id.isupper():
                        try:
                            constants[target.id] = ast.literal_eval(node.value)
                        except (ValueError, SyntaxError):
                            constants[target.id] = "<complex>"

        return ModuleStructure(
            classes=classes,
            functions=functions,
            imports=imports,
            constants=constants
        )

    def _extract_class(self, node: ast.ClassDef) -> ClassInfo:
        """í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ"""

        methods = []
        for item in node.body:
            if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                methods.append(self._extract_method(item))

        base_classes = []
        for base in node.bases:
            if isinstance(base, ast.Name):
                base_classes.append(base.id)
            elif isinstance(base, ast.Attribute):
                try:
                    base_classes.append(ast.unparse(base))
                except Exception:
                    base_classes.append("<complex>")

        return ClassInfo(
            name=node.name,
            docstring=ast.get_docstring(node),
            line_start=node.lineno,
            line_end=node.end_lineno or node.lineno,
            methods=methods,
            base_classes=base_classes
        )

    def _extract_method(self, node: ast.FunctionDef) -> MethodInfo:
        """ë©”ì„œë“œ/í•¨ìˆ˜ ì •ë³´ ì¶”ì¶œ"""

        # íŒŒë¼ë¯¸í„°
        params = []
        for arg in node.args.args:
            param = {'name': arg.arg}
            if arg.annotation:
                try:
                    param['type'] = ast.unparse(arg.annotation)
                except Exception:
                    param['type'] = '<complex>'
            params.append(param)

        # ë¦¬í„´ íƒ€ì…
        return_type = None
        if node.returns:
            try:
                return_type = ast.unparse(node.returns)
            except Exception:
                return_type = '<complex>'

        # ì‹œê·¸ë‹ˆì²˜
        param_strs = []
        for p in params:
            if 'type' in p:
                param_strs.append(f"{p['name']}: {p['type']}")
            else:
                param_strs.append(p['name'])

        sig = f"{'async ' if isinstance(node, ast.AsyncFunctionDef) else ''}def {node.name}({', '.join(param_strs)})"
        if return_type:
            sig += f" -> {return_type}"

        # í˜¸ì¶œ ë° ë³µì¡ë„ë¥¼ í•œ ë²ˆì˜ ìˆœíšŒë¡œ ì¶”ì¶œ
        calls, complexity = self._extract_calls_and_complexity(node)

        return MethodInfo(
            name=node.name,
            signature=sig,
            docstring=ast.get_docstring(node),
            params=params,
            return_type=return_type,
            line_start=node.lineno,
            line_end=node.end_lineno or node.lineno,
            complexity=complexity,
            is_async=isinstance(node, ast.AsyncFunctionDef),
            calls=calls
        )

    def _extract_calls_and_complexity(
        self,
        node: ast.FunctionDef
    ) -> Tuple[List[Dict[str, str]], int]:
        """í˜¸ì¶œ ë° ë³µì¡ë„ë¥¼ í•œ ë²ˆì— ì¶”ì¶œ (ìµœì í™”)"""

        calls = []
        seen: Set[str] = set()
        complexity = 1

        for n in ast.walk(node):
            # ë³µì¡ë„ ê³„ì‚°
            if isinstance(n, (ast.If, ast.For, ast.While, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(n, ast.BoolOp):
                complexity += len(n.values) - 1

            # í˜¸ì¶œ ì¶”ì¶œ
            if isinstance(n, ast.Call):
                call_info = None
                if isinstance(n.func, ast.Attribute):
                    if isinstance(n.func.value, ast.Name) and n.func.value.id == 'self':
                        call_info = {'type': 'method', 'name': n.func.attr}
                    else:
                        try:
                            call_info = {'type': 'external', 'name': ast.unparse(n.func)}
                        except Exception:
                            pass
                elif isinstance(n.func, ast.Name):
                    call_info = {'type': 'function', 'name': n.func.id}

                if call_info:
                    key = f"{call_info['type']}:{call_info['name']}"
                    if key not in seen:
                        calls.append(call_info)
                        seen.add(key)

        return calls, complexity

    async def _ask_ai_about_module(
        self,
        code: str,
        structure: ModuleStructure,
        module_id: str
    ) -> Dict[str, Any]:
        """LLMì—ê²Œ ëª¨ë“ˆ ë¶„ì„ ìš”ì²­ (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸)"""

        # ì½”ë“œ ë¯¸ë¦¬ë³´ê¸°
        code_preview = code[:Config.CODE_PREVIEW_LENGTH]

        # ìš”ì•½ ì •ë³´
        class_names = [c.name for c in structure.classes[:5]]
        method_summary = []
        for cls in structure.classes[:3]:
            for method in cls.methods[:3]:
                method_summary.append(f"{cls.name}.{method.name}()")

        import_modules = [imp['module'] for imp in structure.imports[:8]]

        # Llamaì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a Python code analyzer. Analyze the module and respond with JSON only.<|eot_id|><|start_header_id|>user<|end_header_id|>

Module ID: {module_id}
Classes: {', '.join(class_names) if class_names else 'None'}
Key Methods: {', '.join(method_summary[:5]) if method_summary else 'None'}
Imports: {', '.join(import_modules) if import_modules else 'None'}

Code snippet:
```python
{code_preview}
```

Respond with ONLY valid JSON (no markdown, no extra text):
{{
  "one_liner": "Brief summary in Korean (max 50 chars)",
  "description": "Detailed description in Korean (100-200 chars)",
  "responsibilities": ["responsibility1", "responsibility2", "responsibility3"],
  "key_patterns": ["pattern1", "pattern2"],
  "complexity_level": "low|medium|high",
  "importance": "low|normal|high|critical"
}}
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

        # LLM í˜¸ì¶œ (ê°œì„ ëœ ì¬ì‹œë„ ë¡œì§)
        last_error = None
        for attempt in range(Config.LLM_RETRY_COUNT):
            try:
                response = await self.llm.complete(
                    prompt=prompt,
                    max_tokens=Config.LLM_MAX_TOKENS,
                    temperature=0.1
                )

                # JSON ì •ì œ
                response = response.strip()

                # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                if '```json' in response:
                    response = response.split('```json', 1)[1]
                if '```' in response:
                    response = response.split('```', 1)[0]

                # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                response = response.strip()

                # JSON íŒŒì‹±
                try:
                    result = json.loads(response)
                except json.JSONDecodeError:
                    # ì²« ë²ˆì§¸ {ë¶€í„° ë§ˆì§€ë§‰ }ê¹Œì§€ë§Œ ì¶”ì¶œ
                    start = response.find('{')
                    end = response.rfind('}')
                    if start != -1 and end != -1:
                        response = response[start:end+1]
                        result = json.loads(response)
                    else:
                        raise

                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                required = ['one_liner', 'description', 'responsibilities',
                           'complexity_level', 'importance']

                if not all(k in result for k in required):
                    raise ValueError(f"Missing required fields. Got: {list(result.keys())}")

                # íƒ€ì… ê²€ì¦
                if not isinstance(result['responsibilities'], list):
                    result['responsibilities'] = []
                if 'key_patterns' not in result:
                    result['key_patterns'] = []
                elif not isinstance(result['key_patterns'], list):
                    result['key_patterns'] = []

                return result

            except json.JSONDecodeError as e:
                last_error = f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}"
                if attempt < Config.LLM_RETRY_COUNT - 1:
                    logger.debug(f"      ì¬ì‹œë„ {attempt+1}/{Config.LLM_RETRY_COUNT}")
                    await asyncio.sleep(0.5 * (attempt + 1))
                    continue
            except Exception as e:
                last_error = str(e)
                if attempt < Config.LLM_RETRY_COUNT - 1:
                    logger.debug(f"      LLM ì˜¤ë¥˜ (ì¬ì‹œë„ {attempt+1}): {e}")
                    await asyncio.sleep(0.5 * (attempt + 1))
                    continue

        raise Exception(f"LLM ë¶„ì„ ì‹¤íŒ¨ ({Config.LLM_RETRY_COUNT}íšŒ ì‹œë„): {last_error}")

    def _create_default_description(self, structure: ModuleStructure) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ëª… ìƒì„± (AI ì—†ì´)"""

        if structure.classes:
            class_names = [c.name for c in structure.classes]
            one_liner = f"{', '.join(class_names[:2])} í´ë˜ìŠ¤ í¬í•¨"

            responsibilities = []
            for cls in structure.classes:
                if cls.methods:
                    responsibilities.append(f"{cls.name}: {len(cls.methods)}ê°œ ë©”ì„œë“œ")
        else:
            one_liner = f"{len(structure.functions)}ê°œ í•¨ìˆ˜ í¬í•¨"
            responsibilities = [f.name for f in structure.functions[:3]]

        total_complexity = sum(m.complexity for c in structure.classes for m in c.methods)
        total_complexity += sum(f.complexity for f in structure.functions)

        if total_complexity > 50:
            complexity_level = "high"
        elif total_complexity > 20:
            complexity_level = "medium"
        else:
            complexity_level = "low"

        return {
            "one_liner": one_liner,
            "description": f"ì´ ëª¨ë“ˆì€ {len(structure.classes)}ê°œ í´ë˜ìŠ¤ì™€ {len(structure.functions)}ê°œ í•¨ìˆ˜ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.",
            "responsibilities": responsibilities[:5],
            "key_patterns": [],
            "complexity_level": complexity_level,
            "importance": "normal"
        }

    def _create_module_doc(
        self,
        module_id: str,
        file_path: str,
        structure: ModuleStructure,
        ai_description: Dict[str, Any],
        code: str
    ) -> Dict[str, Any]:
        """ìµœì¢… ëª¨ë“ˆ ë¬¸ì„œ ìƒì„±"""

        # í´ë˜ìŠ¤ ë³€í™˜
        classes_dict = []
        for cls in structure.classes:
            cls_dict = {
                'name': cls.name,
                'docstring': cls.docstring,
                'line_start': cls.line_start,
                'line_end': cls.line_end,
                'base_classes': cls.base_classes,
                'methods': []
            }

            for method in cls.methods:
                cls_dict['methods'].append({
                    'name': method.name,
                    'signature': method.signature,
                    'docstring': method.docstring,
                    'params': method.params,
                    'return_type': method.return_type,
                    'line_start': method.line_start,
                    'line_end': method.line_end,
                    'complexity': method.complexity,
                    'is_async': method.is_async,
                    'calls': method.calls
                })

            classes_dict.append(cls_dict)

        # í•¨ìˆ˜ ë³€í™˜
        functions_dict = []
        for func in structure.functions:
            functions_dict.append({
                'name': func.name,
                'signature': func.signature,
                'docstring': func.docstring,
                'params': func.params,
                'return_type': func.return_type,
                'line_start': func.line_start,
                'line_end': func.line_end,
                'complexity': func.complexity,
                'is_async': func.is_async,
                'calls': func.calls
            })

        doc = {
            'module_id': module_id,
            'file_path': file_path,
            'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),

            'summary': {
                'one_liner': ai_description['one_liner'],
                'description': ai_description['description'],
                'responsibilities': ai_description['responsibilities'],
                'complexity_level': ai_description['complexity_level'],
                'importance': ai_description['importance']
            },

            'structure': {
                'classes': classes_dict,
                'functions': functions_dict,
                'imports': structure.imports,
                'constants': structure.constants
            },

            'metrics': {
                'total_lines': len(code.split('\n')),
                'num_classes': len(structure.classes),
                'num_functions': len(structure.functions),
                'num_methods': sum(len(c.methods) for c in structure.classes),
                'total_complexity': sum(
                    m.complexity for c in structure.classes for m in c.methods
                ) + sum(f.complexity for f in structure.functions)
            }
        }

        # ê³ ë³µì¡ë„ ê²½ê³ 
        high_complexity_methods = []
        for cls in structure.classes:
            for method in cls.methods:
                if method.complexity > Config.HIGH_COMPLEXITY_THRESHOLD:
                    high_complexity_methods.append({
                        'class': cls.name,
                        'method': method.name,
                        'complexity': method.complexity,
                        'lines': [method.line_start, method.line_end]
                    })

        if high_complexity_methods:
            doc['warnings'] = {'high_complexity': high_complexity_methods}

        return doc

    def _create_error_doc(
        self,
        module_id: str,
        file_path: str,
        error: str
    ) -> Dict[str, Any]:
        """ì—ëŸ¬ ë¬¸ì„œ ìƒì„±"""

        return {
            'module_id': module_id,
            'file_path': file_path,
            'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
            'error': error,
            'summary': {
                'one_liner': 'ë¶„ì„ ì‹¤íŒ¨',
                'description': f'ì˜¤ë¥˜: {error}',
                'responsibilities': [],
                'complexity_level': 'unknown',
                'importance': 'unknown'
            },
            'structure': {
                'classes': [],
                'functions': [],
                'imports': [],
                'constants': {}
            },
            'metrics': {
                'total_lines': 0,
                'num_classes': 0,
                'num_functions': 0,
                'num_methods': 0,
                'total_complexity': 0
            }
        }

    def _analyze_relationships(
        self,
        all_modules: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ëª¨ë“ˆ ê°„ ê´€ê³„ ë¶„ì„"""

        import_graph = {}

        for module_id, module_doc in all_modules.items():
            imports = []
            for imp in module_doc['structure']['imports']:
                imp_module = imp['module']
                if self._is_internal_module(imp_module, all_modules):
                    imports.append(imp_module)

            import_graph[module_id] = imports

        cycles = self._find_cycles(import_graph)

        return {
            'import_graph': import_graph,
            'cycles': cycles,
            'statistics': {
                'total_imports': sum(len(v) for v in import_graph.values()),
                'num_cycles': len(cycles)
            }
        }

    def _is_internal_module(
        self,
        module_name: str,
        all_modules: Dict[str, Dict[str, Any]]
    ) -> bool:
        """ë‚´ë¶€ ëª¨ë“ˆì¸ì§€ í™•ì¸"""

        if module_name in all_modules:
            return True

        mod_parts = module_name.split('.')
        for mod_id in all_modules.keys():
            mod_id_parts = mod_id.split('.')
            min_len = min(len(mod_parts), len(mod_id_parts))
            if mod_parts[:min_len] == mod_id_parts[:min_len]:
                return True

        return False

    def _find_cycles(self, graph: Dict[str, List[str]]) -> List[List[str]]:
        """ìˆœí™˜ ì˜ì¡´ì„± ì°¾ê¸°"""

        cycles = []
        visited: Set[str] = set()
        rec_stack: Set[str] = set()

        def dfs(node: str, path: List[str]) -> None:
            if node in rec_stack:
                try:
                    cycle_start = path.index(node)
                    cycle = path[cycle_start:]

                    min_idx = cycle.index(min(cycle))
                    normalized = cycle[min_idx:] + cycle[:min_idx]

                    if normalized not in cycles:
                        cycles.append(normalized)
                except ValueError:
                    pass
                return

            if node in visited:
                return

            visited.add(node)
            rec_stack.add(node)

            for neighbor in graph.get(node, []):
                dfs(neighbor, path + [neighbor])

            rec_stack.remove(node)

        for node in graph:
            if node not in visited:
                dfs(node, [node])

        return cycles

    def _create_index(
        self,
        all_modules: Dict[str, Dict[str, Any]],
        relationships: Dict[str, Any]
    ) -> Dict[str, Any]:
        """index.json ìƒì„±"""

        layers = {}
        for module_id in all_modules.keys():
            parts = module_id.split('.')
            if len(parts) >= 2:
                layer_name = parts[0]
                if layer_name not in layers:
                    layers[layer_name] = {'modules': [], 'count': 0}
                layers[layer_name]['modules'].append(module_id)
                layers[layer_name]['count'] += 1

        modules_summary = []
        for module_id, module_doc in all_modules.items():
            modules_summary.append({
                'id': module_id,
                'role': module_doc['summary']['one_liner'],
                'complexity': module_doc['summary']['complexity_level'],
                'importance': module_doc['summary']['importance'],
                'loc': module_doc['metrics']['total_lines'],
                'detail_file': f"modules/{module_id.replace('.', '_')}.json"
            })

        importance_order = {'critical': 0, 'high': 1, 'normal': 2, 'low': 3}
        modules_summary.sort(
            key=lambda m: (importance_order.get(m['importance'], 9), -m['loc'])
        )

        index = {
            'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
            'project': self.project_root.name,
            'total_modules': len(all_modules),
            'total_loc': sum(m['metrics']['total_lines'] for m in all_modules.values()),

            'layers': layers,
            'modules': modules_summary,

            'statistics': {
                'total_classes': sum(m['metrics']['num_classes'] for m in all_modules.values()),
                'total_functions': sum(m['metrics']['num_functions'] for m in all_modules.values()),
                'total_imports': relationships['statistics']['total_imports'],
                'circular_dependencies': len(relationships['cycles'])
            },

            'warnings': []
        }

        if relationships['cycles']:
            index['warnings'].append({
                'type': 'circular_dependency',
                'count': len(relationships['cycles']),
                'severity': 'medium',
                'cycles': relationships['cycles']
            })

        high_complexity_modules = [
            m['module_id'] for m in all_modules.values()
            if m['summary']['complexity_level'] == 'high'
        ]
        if high_complexity_modules:
            index['warnings'].append({
                'type': 'high_complexity',
                'modules': high_complexity_modules,
                'severity': 'low'
            })

        return index

    def _save_documentation(
        self,
        all_modules: Dict[str, Dict[str, Any]],
        relationships: Dict[str, Any],
        index: Dict[str, Any]
    ):
        """ë¬¸ì„œ ì €ì¥"""

        def save_json(path: Path, data: Dict[str, Any]) -> None:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
            futures = []

            # index.json
            index_path = self.output_dir / "index.json"
            futures.append(executor.submit(save_json, index_path, index))

            # ëª¨ë“ˆ ìƒì„¸
            for module_id, module_doc in all_modules.items():
                filename = f"{module_id.replace('.', '_')}.json"
                module_path = self.output_dir / "modules" / filename
                futures.append(executor.submit(save_json, module_path, module_doc))

            # ì˜ì¡´ì„±
            rel_path = self.output_dir / "relationships" / "dependencies.json"
            futures.append(executor.submit(save_json, rel_path, relationships))

            for future in futures:
                future.result()

        logger.info(f"   âœ“ index.json")
        logger.info(f"   âœ“ {len(all_modules)}ê°œ ëª¨ë“ˆ ë¬¸ì„œ")
        logger.info(f"   âœ“ dependencies.json")


# =============================================================================
# CLI ì§„ì…ì 
# =============================================================================

async def main():
    """CLI ì§„ì…ì """

    import argparse

    parser = argparse.ArgumentParser(
        description='Python ì½”ë“œë² ì´ìŠ¤ ìë™ ë¶„ì„ ë„êµ¬ (vLLM + Llama)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # vLLM ì„œë²„ ë¨¼ì € ì‹œì‘:
  python -m vllm.entrypoints.openai.api_server \\
      --model meta-llama/Llama-3.2-3B-Instruct \\
      --port 8000

  # ë¶„ì„ ì‹¤í–‰:
  python module_analyzer_vllm.py /path/to/project
  python module_analyzer_vllm.py /path/to/project --target src
  python module_analyzer_vllm.py /path/to/project --llm-url http://localhost:8000
  python module_analyzer_vllm.py /path/to/project --no-ai  # AI ì—†ì´
        """
    )

    parser.add_argument('project_root', help='í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬')
    parser.add_argument('--target', default='.', help='ë¶„ì„í•  í•˜ìœ„ ë””ë ‰í† ë¦¬')
    parser.add_argument('--no-ai', action='store_true', help='AI ë¶„ì„ ë¹„í™œì„±í™”')
    parser.add_argument('--no-cache', action='store_true', help='ìºì‹± ë¹„í™œì„±í™”')
    parser.add_argument('--exclude', nargs='+', help='ì œì™¸í•  íŒ¨í„´')
    parser.add_argument('--llm-url', default='http://localhost:8000', help='vLLM ì„œë²„ URL')
    parser.add_argument('--llm-model', default='meta-llama/Llama-3.2-3B-Instruct', help='ëª¨ë¸ëª…')

    args = parser.parse_args()

    # ë¶„ì„ ì‹¤í–‰
    analyzer = ModuleAnalyzer(
        project_root=args.project_root,
        use_ai=not args.no_ai,
        use_cache=not args.no_cache,
        llm_url=args.llm_url,
        llm_model=args.llm_model
    )

    await analyzer.analyze_project(
        target_dir=args.target,
        exclude_patterns=args.exclude
    )


if __name__ == '__main__':
    asyncio.run(main())
